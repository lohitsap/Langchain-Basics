from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser

load_dotenv()

# Define the model
llm = HuggingFaceEndpoint(
    repo_id="google/gemma-2-2b-it",
    task="text-generation"
)

model = ChatHuggingFace(llm=llm)

parser = JsonOutputParser()

template = PromptTemplate(
    template='Give me 5 facts about {topic} \n {format_instruction}',
    #  # {format_instruction} : this tells additional instructions we send to LLM as to what kind of output we expect from LLM
    input_variables=['topic'], 
    partial_variables={'format_instruction': parser.get_format_instructions()}
) # get_format_instructions : this is the instruction we are sending to LLM

chain = template | model | parser

result = chain.invoke({'topic':'black hole'})

print(result)

'''
with the help of jsonoutputparser, we are able to ger json output from the LLM.
But we are not able to define the schema.

Example of schema :

What if i want a structure like this :
{
    "fact1": "information on fact 1",
    "fact2": "information on fact 2",
    "fact3": "information on fact 3",
}

So I cannot define a custom schema in this case.
I have to depend on the json output generated by the parser only. This is the biggest flaw.

This is where StructuredOutputParser comes into play.

You can go to this file : structured_output_parser.py
'''