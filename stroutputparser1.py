# we need to import StrOutputParser from langchain_core.output_parsers
# in this code we will see how can we generate the code using chaining method 

from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
load_dotenv()
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

model = ChatOpenAI()


# give Prompt 1 : Detailed report on a topic
template1 = PromptTemplate(
    template="Generate a detailed report on the topic: {topic}",
    input_variables=["topic"]
)

# give Prompt 2 : Extract response and take Summary of the report generated by LLM
template2 = PromptTemplate(
    template="Write a 5 line summary on the following text. /n {text}",
    input_variables=["text"]
)

# Start with creating a parser
parser = StrOutputParser()

# now form a chain (chain is a pieline where we execute the entire flow)

chain = template1 | model | parser | template2 | model | parser

'''
Understanding chain code : 
We start with template1 : where the user sends some topic.
Then we send the prompt(topic) to the model. Our model will process the result. But the result has metadata in which 
we are not intersted. We only want the textual content.
This is done by parser which extracts the detailed report from the entire metadata.
Now we have to take the report and send it to template 2 (this will generate 2nd prompt)
Now we have to send 2nd prompt to the model and again it will generate a 5 line summary with lot of metadata.
We dont need metadata only the summary, so we send it again to the parser.
'''


result = chain.invoke({"topic": "black holes"})

print(result)

'''
You can see just how we received the final result in stroutputparser.py code file
we got it here too, but with much easier method called as chaining.

Why was this possible ? Because of parsers.

After this , we will understand json output parser. (jsonoutputparser1.py)

'''