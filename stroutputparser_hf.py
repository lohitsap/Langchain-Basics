# in case you want to understand stroutputparser code and how it works with huggingface
# use this


# first code in stroutput parser series
# we will use dynamic prompt template to generate a report on a topic

'''
Structure :

give Prompt 1 : Detailed report on a topic

give Prompt 2 : Extract response and take Summary of the report generated by LLM

'''

from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
from dotenv import load_dotenv
load_dotenv()
from langchain_core.prompts import PromptTemplate


# now we know this LLM cannot generate structured output by default.
llm = HuggingFaceEndpoint(
    repo_id='google/gemma-2-2b-it', # repo id : you can copy paste from huggingface models page
    task='text-generation')

model = ChatHuggingFace(llm=llm)


# give Prompt 1 : Detailed report on a topic
template1 = PromptTemplate(
    template="Generate a detailed report on the topic: {topic}",
    input_variables=["topic"]
)

# give Prompt 2 : Extract response and take Summary of the report generated by LLM
template2 = PromptTemplate(
    template="Write a 5 line summary on the following text. /n {text}",
    input_variables=["text"]
)

# first send template1 in the prompt

prompt1 = template1.invoke({"topic": "black holes"})
# send this to model now
result = model.invoke(prompt1)

# make prompt 2
prompt2 = template2.invoke({"text": result.content})
# send this 2nd prompt to the model

result1 = model.invoke(prompt2)

# finally print the result
print(result1.content) 